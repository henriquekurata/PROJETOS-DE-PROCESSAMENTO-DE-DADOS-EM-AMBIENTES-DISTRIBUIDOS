# ğŸš€ ***Pipeline para processamento de dados com banco relacional, Airbyte, Databricks e anÃ¡lise com SQL***

## ğŸ“– **DescriÃ§Ã£o do Projeto:**
Este projeto envolve a criaÃ§Ã£o de um pipeline de dados que conecta um banco de dados relacional local em PostgreSQL com um Data Lakehouse na nuvem utilizando o Airbyte como ferramenta de ETL. O pipeline extrai os dados do banco de dados local, processa-os no Databricks e armazena-os em um bucket S3 na AWS. Os dados sÃ£o entÃ£o analisados com SQL no Databricks, onde tambÃ©m foram criados grÃ¡ficos e dashboards para visualizaÃ§Ã£o.


## Funcionalidades Principais
1. **CriaÃ§Ã£o do banco de dados local**: Banco PostgreSQL criado e gerenciado em um container Docker, com tabelas de usuÃ¡rios e cidades e relacionamento entre elas.
2. **IntegraÃ§Ã£o de dados**: UtilizaÃ§Ã£o do Airbyte para extrair dados do PostgreSQL e carregar no Databricks, configurando fontes e destinos de forma automatizada.
3. **Armazenamento em Data Lake**: Os dados processados foram armazenados no bucket S3 na AWS, acessÃ­veis diretamente pela interface Databricks.
4. **AnÃ¡lise de dados com SQL**: Consultas SQL realizadas no Databricks para combinar e visualizar os dados, com a criaÃ§Ã£o de grÃ¡ficos e dashboards.


## ğŸ› ï¸ Ferramentas Utilizadas
- **Docker**: Ferramenta de containers utilizada para criar e gerenciar o ambiente local.
- **PostgreSQL**: Banco de dados relacional usado como fonte de dados.
- **Airbyte**: Ferramenta de integraÃ§Ã£o de dados para sincronizar a fonte PostgreSQL com o Data Lakehouse no Databricks.
- **Databricks**: Plataforma em nuvem usada para processamento e anÃ¡lise de dados.
- **AWS**: Infraestrutura em nuvem utilizada para armazenamento no S3 e execuÃ§Ã£o de clusters EC2.


## ğŸ“‹ **DescriÃ§Ã£o do Processo**
* Banco de dados Relacional (local-Docker-PostgreSQL) > Airbyte (Local-Docker)> Data Lakehouse (Nuvem-Databricks-AWS)


## ğŸ’» **Comandos:** 

### Fonte de dados

#### Instalar o Docker de acordo com o sistema operacional da mÃ¡quina local

#### Executar o comando para baixar a imagem e criar o container:

docker run --name db-dsa-fonte -p 5432:5432 -e POSTGRES_USER=dbadmin -e POSTGRES_PASSWORD=dbadmin123 -e POSTGRES_DB=postgresDSADB -d postgres

Container Docker = db-dsa-fonte

Banco de dados = postgresDSADB

Schema = dbadmin

---

### Construindo e carregando o banco de dados local como origem dos dados

#Instale o pgAdmin, crie a conexÃ£o para o banco de dados e execute as instruÃ§Ãµes SQL abaixo
https://www.pgadmin.org

#### Criar schema
``` sql
CREATE SCHEMA dbadmin AUTHORIZATION dbadmin;
```

#### Criar tabelas
```sql
CREATE TABLE dbadmin.tb_usuarios
(
    id_usuario integer NOT NULL,
    nome_usuario character varying(50),
    cod_cidade character varying(5),
    PRIMARY KEY (id_usuario)
);
```

```sql
CREATE TABLE dbadmin.tb_cidades
(
    codigo_cidade character varying(5),
    nome_cidade character varying(50),
    PRIMARY KEY (codigo_cidade)
);
```

#### Carregar dados
```sql
INSERT INTO dbadmin.tb_cidades(codigo_cidade, nome_cidade)
VALUES ('FOR01', 'Fortaleza');

INSERT INTO dbadmin.tb_cidades(codigo_cidade, nome_cidade)
VALUES ('BLU01', 'Blumenau');

INSERT INTO dbadmin.tb_cidades(codigo_cidade, nome_cidade)
VALUES ('UBA01', 'Ubatuba');

INSERT INTO dbadmin.tb_usuarios(id_usuario, nome_usuario, cod_cidade)
VALUES (1001, 'Bob Silva', 'BLU01');

INSERT INTO dbadmin.tb_usuarios(id_usuario, nome_usuario, cod_cidade)
VALUES (1002, 'Monica Teixeira', 'BLU01');

INSERT INTO dbadmin.tb_usuarios(id_usuario, nome_usuario, cod_cidade)
VALUES (1003, 'Josenildo Farias', 'FOR01');

INSERT INTO dbadmin.tb_usuarios(id_usuario, nome_usuario, cod_cidade)
VALUES (1004, 'Maria Joy', 'UBA01');

INSERT INTO dbadmin.tb_usuarios(id_usuario, nome_usuario, cod_cidade)
VALUES (1005, 'Alex Tavares', 'FOR01');
```


#### Criar chave estrangeira
```
ALTER TABLE dbadmin.tb_usuarios
    ADD CONSTRAINT "FK_CIDADE" FOREIGN KEY (cod_cidade)
    REFERENCES dbadmin.tb_cidades (codigo_cidade)
    ON UPDATE NO ACTION
    ON DELETE NO ACTION;
```

---

### Instalando o Airbyte localmente no docker 

#### Execute os comandos abaixo para instalar o Airbyte

#Se necessÃ¡rio, instale o Git: https://git-scm.com/book/en/v2/Getting-Started-Installing-Git (De acordo com o sistema operacional)

git clone https://github.com/airbytehq/airbyte.git

cd airbyte

docker-compose up



### Preparar ambiente em nuvem com Databricks

Criar conta para Databricks e AWS

No Databricks criar um bucket S3 (para armazenamento dos dados processados) para configurar a inicializaÃ§Ã£o do CloudFormation (CloudFormation > Create Stack > Workspace configuration > Data bucket name)

Criar a Stack do CloudFormation com arquivo yaml da prÃ³pria Databricks
Acessar Databricks > Workspace > Abrir as configuraÃ§Ãµes da Worskspace > Acessar Compute > Criar Cluster de mÃ¡quinas EC2 de acordo com o hardware necessÃ¡rio

Obs: Dentro do cluster EC2 pela Databricks existem outras configuraÃ§Ãµes que podem ser acessadas: notebooks, libraries, event log, spark ui, driver logs, metrics, apps, spark cluster UI-Master



### ConfiguraÃ§Ã£o do Airbyte

#### Fonte - PostgreSQL

Configurar a fonte (PostgresLocal) com os dados do conteiner Postgres

Clicar em setup source

#### Destino - Databricks

Configurar o destino com os dados do cluster criado na Databricks: 

Server Hostname/ Port / HTTP Path: Acessar Workspace > Compute > Cluster > Advanced options > JDBC/ODBC > CÃ³pia dos detalhes de conexÃ£o para o Airbyte

Aceitar os termos de uso: Agree to the databricks JDBC

Access Token: Ãcone de usuÃ¡rio > Settings > User > Developer > Access Tokens > Generate new token 
Database Schema: dbadmin (mesmo nome da fonte de dados)

S3 bucket name: nome bucket S3

S3 bucket path: criar uma pasta com o nome "dados" no bucket S3

S3 bucket region: Us-east-2

S3 Access Key id / S3 secret Access Key: Security credentials > Criar chave

S3 Filename: {date} (Ã‰ importante fazer essa configuraÃ§Ã£o para tornar possÃ­vel a execuÃ§Ã£o do pipeline por vÃ¡rias vezes)

Clicar em setup destination

---

### Configurando a conexÃ£o Airbyte

Selecionar sorce > selecionar destination > editar transfer (agendamento de execuÃ§Ã£o) > destination namespace (mirror: levarÃ¡ a mesma estrutura a para o destino) > destination Stream Prefix (prefixo para diferenciar o destino) 

Full refresh/Overwrite: Apaga e grava todos os dados novamente

Incremental/Append: Adicionar apenas os dados que ainda nÃ£o existem no destino (para isso Ã© importante selecionar a chave primÃ¡ria dessa tabela)

Clicar em sync now (ExecuÃ§Ã£o do pipeline de integraÃ§Ã£o de dados)

---

### AnÃ¡lise de dados com SQL

Abrir Workspace :

#### Listar o conteÃºdo no data lake:
```
dbutils.fs.ls("s3://projeto5-dsa-analytics/dados/dbadmin/") - Entre parenteses Ã© a URI que estÃ£o as duas tabelas criadas no S3
```

#### Ler a tabela e gravar em um dataframe:
```python
df = spark.read.load("s3://projeto5-dsa-analytics/dados/dbadmin/p5_tb_usuarios")
display(df) 
```

#### Fazendo Join das duas tabelas com SQL
```sql
%sql
SELECT nome_usuario, nome_cidade
FROM dbadmin.p5_tb_usuarios, dbadmin.p5_tb_cidades
WHERE dbadmin.p5_tb_cidades.codigo_cidade = dbadmin.p5_tb_usuarios.cod_cidade;
```

#### Criar grÃ¡fico
```sql
%sql
SELECT nome_cidade, COUNT(*)
FROM dbadmin.p5_tb_usuarios, dbadmin.p5_tb_cidades
WHERE dbadmin.p5_tb_cidades.codigo_cidade = dbadmin.p5_tb_usuarios.cod_cidade
GROUP BY nome_cidade;
```

Para a criaÃ§Ã£o do grÃ¡fico basta acessar: sqldf > Table > + > ConfiguraÃ§Ãµes do grÃ¡fico > Save

#### Criar Dashboard

Acessar : Tabela criada > Show in Dashboard ou add to dashboard > ConfiguraÃ§Ãµes Dashboard

Acessar o grÃ¡dio > Add to dashboard 

Assim teremos o painel com as tabelas e o grÃ¡fico em visualizaÃ§Ã£o Ãºnica


---
## Contato

Se tiver dÃºvidas ou sugestÃµes sobre o projeto, entre em contato comigo:

- ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/henrique-k-32967a2b5/)
- ğŸ± [GitHub](https://github.com/henriquekurata?tab=overview&from=2024-09-01&to=2024-09-01)
