# üöÄ ***Processamento distribu√≠do na nuvem com Amazon EMR no EC2 com PySpark***

## üìñ **Descri√ß√£o do Projeto:**
Este projeto demonstra o uso de um cluster Amazon EMR para processar dados distribu√≠dos utilizando PySpark. S√£o realizados diversos passos, desde a configura√ß√£o do cluster at√© a execu√ß√£o de pipelines de processamento de texto e a manipula√ß√£o de dados no HDFS.



## üõ†Ô∏è Ferramentas Utilizadas:
- Amazon EMR
- EC2
- HDFS
- PySpark


## üìã **Descri√ß√£o do Processo:**

- Cria√ß√£o de um cluster EMR com Spark na AWS.
- Acesso remoto ao cluster via SSH para configura√ß√£o.
- Extra√ß√£o de dados de texto da internet.
- Transfer√™ncia dos dados para o HDFS (Sistema de Arquivos Distribu√≠do Hadoop).
- Processamento dos dados com PySpark, incluindo limpeza e manipula√ß√£o de texto.
- Armazenamento dos resultados no HDFS.
- Transfer√™ncia dos resultados para o sistema local e o Amazon S3.



## üíª **Comandos:** 

### Cria√ß√£o do Cluster EMR com Spark:
1. Acesse **AWS > Amazon EMR > Criar Cluster**.
2. Selecione a vers√£o do Amazon EMR com Spark.
3. N√£o use o cat√°logo de dados do AWS Glue.
4. Selecione o sistema operacional **Linux** e aplique as atualiza√ß√µes mais recentes.
5. Defina o grupo de inst√¢ncias (m√°quinas prim√°rias, n√∫cleos e tarefas).
6. Configure o tamanho do cluster e defina rede (VPC, Subnet) e grupos de seguran√ßa.
7. Defina a√ß√µes de bootstrap, logs no S3 e as chaves do EC2 para acesso remoto.
8. Crie o Cluster.

---

### Acessando o Cluster Remotamente:
1. Acesse o **Amazon EMR > Cluster > Resumo** e conecte ao n√≥ prim√°rio via **SSH**.
2. Configure o Putty com o endere√ßo do cluster e as chaves **ppk**.
3. Caso ocorra erro de timeout, edite as regras de entrada do firewall para liberar portas necess√°rias (TCP 22 e TCPs personalizados).

Obs: Caso haja problemas de acesso como "timeout" basta acessar o cluster > propriedades > Rede e seguran√ßa > Grupos de seguran√ßa do EC2 (firewall) > N√≥ prim√°rio > 
Editar regras de entrada (Criar regra: SSH > TCP 22 > Qualquer 0.0.0.0./0)


Acessar cluster EMR > Aplicativos > UIs de aplicativo no n√≥ prim√°rio > Para acessar essas portas √© necess√°rio liberar o acesso > 
Editar regras de entrada > (Todos os TCPs > Presonalizado > Qualquer > 0.0.0.0./0) > Salvar regras

---

### Executandos os pipelines no cluster:

#### **Tarefa 1 - Extrair Dados de Texto**

#Usaremos a url no formato abaixo (exemplo):
#https://www.gutenberg.org/files/136/136.txt

1. Conecte-se ao cluster via **SSH**.

2. Crie uma pasta para armazenar os dados:
    ```bash
    mkdir dsa-dados-entrada-local
    cd dsa-dados-entrada-local
    vi tarefa1.sh
    ```
3. Adicione o seguinte script ao arquivo:
    ```bash
    #!/bin/bash 
    for i in {1340..1400} 
    do 
        wget "http://www.gutenberg.org/files/$i/$i.txt" 
    done
    ```
4. Execute o script para baixar os dados:
    ```bash
    chmod +x tarefa1.sh
    ./tarefa1.sh
    ```
---

### Tarefa 2 - Mover os Dados de Texto Para o Sistema de Arquivos Distribu√≠do

#### Acesse a pasta home do usu√°rio no Cluster EMR

cd ~

#Verifica se o HDFS est√° dispon√≠vel e ent√£o cria uma pasta no 

HDFS:

hdfs dfs -ls /

hdfs dfs -mkdir /user/hadoop/dsa-dados-entrada-dfs

hdfs dfs -ls /user/hadoop

#Copie os arquivos de texto no formato txt do sistema de arquivos local para o sistema de arquivos distribu√≠do:

hdfs dfs -put dsa-dados-entrada-local/*.txt dsa-dados-entrada-dfs

#### Verifique se os arquivos est√£o agora no ambiente distribu√≠do:
```
hdfs dfs -ls /

hdfs dfs -ls /user

hdfs dfs -ls /user/hadoop

hdfs dfs -ls /user/hadoop/dsa-dados-entrada-dfs

hdfs dfs -ls /user/hadoop/dsa-dados-entrada-dfs/1340.txt
```
---

### Tarefa 3 - Criar e Executar o Pipeline

#Acesse a pasta home do usu√°rio no Cluster EMR

cd ~

#Crie o arquivo para o script do Pipeline:

vi projeto6.py

#Coloque o conte√∫do abaixo no script:

```py
# Imports
import re
import string
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import ArrayType, StringType

# Cria uma sess√£o Spark
spark = SparkSession.builder \
    .appName("DSAProjeto6") \
    .getOrCreate()

# Define uma fun√ß√£o para limpar o texto
def limpa_texto(text):
    
    # Cria um REGEX
    # https://docs.python.org/3/library/re.html
    regex = re.compile('[%s]' % re.escape(string.punctuation))

    # Aplica o REGEX e remove pontua√ß√£o
    texto_limpo = regex.sub('', text)
    
    # Converte para min√∫sculas e divide o texto em palavras
    words = texto_limpo.lower().split()
    
    # Cria uma lista de stopwords (palavras que n√£o s√£o relevantes em PLN)
    stopwords = set(["a", "an", "and", "are", "as", "at", "be", "by", "for", "from", "has", "he", "in", "is", "it", "its", "of", "on", "that", "the", "to", "was", "were", "will", "with"])
    
    # Remove stopwords
    texto_limpo_final = [word for word in words if word not in stopwords]
    
    return texto_limpo_final

# Define uma UDF para limpar o texto
# UDFs s√£o usados para estender as fun√ß√µes Python e reutilizar essas fun√ß√µes em v√°rios DataFrames
udf_limpa_texto = udf(limpa_texto, ArrayType(StringType()))

# Ler os arquivos de texto da pasta
# Substitua 'hdfs://[host]:[port]/[path]' pelo caminho correto para sua pasta no HDFS
# Use o endere√ßo do seu Cluster EMR (S√≥ alterar "/ip-172-31-2-107" pelo endere√ßo que est√° na linha de comando do acesso ao cluster referente ao n√≥ prim√°rio
df_texto_entrada = spark.read.text("hdfs://ip-172-31-2-107.us-east-2.compute.internal:8020/user/hadoop/dsa-dados-entrada-dfs")

# Limpa o texto e divide cada frase em uma lista de palavras
df_texto_saida = df_texto_entrada.withColumn("lista_palavras", udf_limpa_texto(df_texto_entrada["value"]))

# Gravar as palavras resultantes em um arquivo de sa√≠da
# Substitua 'hdfs://[host]:[port]/[path_saida]' pelo caminho correto para sua pasta de sa√≠da no HDFS
# Use o endere√ßo do seu Cluster EMR 
df_texto_saida.write.mode("overwrite").json("hdfs://ip-172-31-2-107.us-east-2.compute.internal:8020/user/hadoop/dsa-dados-saida-dfs")

# Parar a sess√£o Spark
spark.stop()

```

#### Execute o script e submete o job para o Spark:
```
spark-submit projeto6.py
```

---

### Tarefa 4 - Manipular os Dados de Texto Ap√≥s o Processamento

#### Verfique se o resultado foi gravado no HDFS:

hdfs dfs -ls /user/hadoop/dsa-dados-saida-dfs/*

#### Crie uma pasta para gravar os dados de sa√≠da no sistema de arquivos local:

mkdir dsa-dados-saida-local

#Entre na pasta:

cd dsa-dados-saida-local

#### Copie os arquivos de sa√≠da do sistema de arquivos distribu√≠do para o sistema de arquivos local:

hdfs dfs -get /user/hadoop/dsa-dados-saida-dfs/*


---

### Tarefa 5 - Combinar os Arquivos de Sa√≠da e Obter o Resultado do Pipeline no Cluster EMR

#### Acesse a pasta com os dados de sa√≠da no sistema local no Cluster EMR:

cd dsa-dados-saida-local

#Crie o arquivo sh:

vi combina_json.sh

#### Coloque no arquivo o conte√∫do abaixo para combinar os arquivos JSON e gerar um √∫nico arquivo de sa√≠da:
```
#!/bin/bash
# Encontra todos os arquivos JSON e os passa para jq para combina√ß√£o em um √∫nico array
find . -name "*.json" -print0 | xargs -0 jq -c '.' | jq -s '.' > dsa-resultado.json

# Altere a permiss√£o do arquivo para torn√°-lo um execut√°vel:
chmod +x combina_json.sh

# Execute o script:
./combina_json.sh

# Verifique o resultado:
cat dsa-resultado.json

# Zip do arquivo:
zip dsa-resultado.zip dsa-resultado.json

# Copia o arquivo zip para o S3: Endere√ßo "aws-logs-890582101704-us-east-2" retirado da p√°gina do Amazon S3 (URI do S3)
aws s3 cp dsa-resultado.zip s3://aws-logs-890582101704-us-east-2/elasticmapreduce/dsa-resultado.zip
```

#### Acesse o console do S3 e fa√ßa o download do arquivo para a m√°quina local.
   ```bash
    zip dsa-resultado.zip dsa-resultado.json
    aws s3 cp dsa-resultado.zip s3://aws-logs-890582101704-us-east-2/elasticmapreduce/dsa-resultado.zip
    ```

---
## Contato

Se tiver d√∫vidas ou sugest√µes sobre o projeto, entre em contato comigo:

- üíº [LinkedIn](https://www.linkedin.com/in/henrique-k-32967a2b5/)
- üê± [GitHub](https://github.com/henriquekurata?tab=overview&from=2024-09-01&to=2024-09-01)
