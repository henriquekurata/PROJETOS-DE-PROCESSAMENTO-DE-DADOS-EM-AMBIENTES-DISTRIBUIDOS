# ğŸš€ ***Processamento distribuÃ­do na nuvem com Amazon EMR no EC2 com PySpark***

## ğŸ“– **DescriÃ§Ã£o do Projeto:**
Este projeto demonstra o uso de um cluster Amazon EMR para processar dados distribuÃ­dos utilizando PySpark. SÃ£o realizados diversos passos, desde a configuraÃ§Ã£o do cluster atÃ© a execuÃ§Ã£o de pipelines de processamento de texto e a manipulaÃ§Ã£o de dados no HDFS.



## ğŸ› ï¸ Ferramentas Utilizadas:
- Amazon EMR
- EC2
- HDFS
- PySpark


## ğŸ“‹ **DescriÃ§Ã£o do Processo:**

- CriaÃ§Ã£o de um cluster EMR com Spark na AWS.
- Acesso remoto ao cluster via SSH para configuraÃ§Ã£o.
- ExtraÃ§Ã£o de dados de texto da internet.
- TransferÃªncia dos dados para o HDFS (Sistema de Arquivos DistribuÃ­do Hadoop).
- Processamento dos dados com PySpark, incluindo limpeza e manipulaÃ§Ã£o de texto.
- Armazenamento dos resultados no HDFS.
- TransferÃªncia dos resultados para o sistema local e o Amazon S3.



## ğŸ’» **Comandos:** 

### CriaÃ§Ã£o do Cluster EMR com Spark:
1. Acesse **AWS > Amazon EMR > Criar Cluster**.
2. Selecione a versÃ£o do Amazon EMR com Spark.
3. NÃ£o use o catÃ¡logo de dados do AWS Glue.
4. Selecione o sistema operacional **Linux** e aplique as atualizaÃ§Ãµes mais recentes.
5. Defina o grupo de instÃ¢ncias (mÃ¡quinas primÃ¡rias, nÃºcleos e tarefas).
6. Configure o tamanho do cluster e defina rede (VPC, Subnet) e grupos de seguranÃ§a.
7. Defina aÃ§Ãµes de bootstrap, logs no S3 e as chaves do EC2 para acesso remoto.
8. Crie o Cluster.

---

### Acessando o Cluster Remotamente:
1. Acesse o **Amazon EMR > Cluster > Resumo** e conecte ao nÃ³ primÃ¡rio via **SSH**.
2. Configure o Putty com o endereÃ§o do cluster e as chaves **ppk**.
3. Caso ocorra erro de timeout, edite as regras de entrada do firewall para liberar portas necessÃ¡rias (TCP 22 e TCPs personalizados).

Obs: Caso haja problemas de acesso como "timeout" basta acessar o cluster > propriedades > Rede e seguranÃ§a > Grupos de seguranÃ§a do EC2 (firewall) > NÃ³ primÃ¡rio > 
Editar regras de entrada (Criar regra: SSH > TCP 22 > Qualquer 0.0.0.0./0)


Acessar cluster EMR > Aplicativos > UIs de aplicativo no nÃ³ primÃ¡rio > Para acessar essas portas Ã© necessÃ¡rio liberar o acesso > 
Editar regras de entrada > (Todos os TCPs > Presonalizado > Qualquer > 0.0.0.0./0) > Salvar regras

---

### Executandos os pipelines no cluster:

#### **Tarefa 1 - Extrair Dados de Texto**

#Usaremos a url no formato abaixo (exemplo):
#https://www.gutenberg.org/files/136/136.txt

1. Conecte-se ao cluster via **SSH**.

2. Crie uma pasta para armazenar os dados:
    ```bash
    mkdir dsa-dados-entrada-local
    cd dsa-dados-entrada-local
    vi tarefa1.sh
    ```
3. Adicione o seguinte script ao arquivo:
    ```bash
    #!/bin/bash 
    for i in {1340..1400} 
    do 
        wget "http://www.gutenberg.org/files/$i/$i.txt" 
    done
    ```
4. Execute o script para baixar os dados:
    ```bash
    chmod +x tarefa1.sh
    ./tarefa1.sh
    ```
---

### Tarefa 2 - Mover os Dados de Texto Para o Sistema de Arquivos DistribuÃ­do

#### Acesse a pasta home do usuÃ¡rio no Cluster EMR

cd ~

#Verifica se o HDFS estÃ¡ disponÃ­vel e entÃ£o cria uma pasta no 

HDFS:

hdfs dfs -ls /

hdfs dfs -mkdir /user/hadoop/dsa-dados-entrada-dfs

hdfs dfs -ls /user/hadoop

#Copie os arquivos de texto no formato txt do sistema de arquivos local para o sistema de arquivos distribuÃ­do:

hdfs dfs -put dsa-dados-entrada-local/*.txt dsa-dados-entrada-dfs

#### Verifique se os arquivos estÃ£o agora no ambiente distribuÃ­do:
```
hdfs dfs -ls /

hdfs dfs -ls /user

hdfs dfs -ls /user/hadoop

hdfs dfs -ls /user/hadoop/dsa-dados-entrada-dfs

hdfs dfs -ls /user/hadoop/dsa-dados-entrada-dfs/1340.txt
```
---

### Tarefa 3 - Criar e Executar o Pipeline

#Acesse a pasta home do usuÃ¡rio no Cluster EMR

cd ~

#Crie o arquivo para o script do Pipeline:

vi projeto6.py

#Coloque o conteÃºdo abaixo no script:

```py
# Imports
import re
import string
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import ArrayType, StringType

# Cria uma sessÃ£o Spark
spark = SparkSession.builder \
    .appName("DSAProjeto6") \
    .getOrCreate()

# Define uma funÃ§Ã£o para limpar o texto
def limpa_texto(text):
    
    # Cria um REGEX
    # https://docs.python.org/3/library/re.html
    regex = re.compile('[%s]' % re.escape(string.punctuation))

    # Aplica o REGEX e remove pontuaÃ§Ã£o
    texto_limpo = regex.sub('', text)
    
    # Converte para minÃºsculas e divide o texto em palavras
    words = texto_limpo.lower().split()
    
    # Cria uma lista de stopwords (palavras que nÃ£o sÃ£o relevantes em PLN)
    stopwords = set(["a", "an", "and", "are", "as", "at", "be", "by", "for", "from", "has", "he", "in", "is", "it", "its", "of", "on", "that", "the", "to", "was", "were", "will", "with"])
    
    # Remove stopwords
    texto_limpo_final = [word for word in words if word not in stopwords]
    
    return texto_limpo_final

# Define uma UDF para limpar o texto
# UDFs sÃ£o usados para estender as funÃ§Ãµes Python e reutilizar essas funÃ§Ãµes em vÃ¡rios DataFrames
udf_limpa_texto = udf(limpa_texto, ArrayType(StringType()))

# Ler os arquivos de texto da pasta
# Substitua 'hdfs://[host]:[port]/[path]' pelo caminho correto para sua pasta no HDFS
# Use o endereÃ§o do seu Cluster EMR (SÃ³ alterar "/ip-172-31-2-107" pelo endereÃ§o que estÃ¡ na linha de comando do acesso ao cluster referente ao nÃ³ primÃ¡rio
df_texto_entrada = spark.read.text("hdfs://ip-172-31-2-107.us-east-2.compute.internal:8020/user/hadoop/dsa-dados-entrada-dfs")

# Limpa o texto e divide cada frase em uma lista de palavras
df_texto_saida = df_texto_entrada.withColumn("lista_palavras", udf_limpa_texto(df_texto_entrada["value"]))

# Gravar as palavras resultantes em um arquivo de saÃ­da
# Substitua 'hdfs://[host]:[port]/[path_saida]' pelo caminho correto para sua pasta de saÃ­da no HDFS
# Use o endereÃ§o do seu Cluster EMR 
df_texto_saida.write.mode("overwrite").json("hdfs://ip-172-31-2-107.us-east-2.compute.internal:8020/user/hadoop/dsa-dados-saida-dfs")

# Parar a sessÃ£o Spark
spark.stop()

```

#### Execute o script e submete o job para o Spark:
```
spark-submit projeto6.py
```

---

### Tarefa 4 - Manipular os Dados de Texto ApÃ³s o Processamento

#### Verfique se o resultado foi gravado no HDFS:

hdfs dfs -ls /user/hadoop/dsa-dados-saida-dfs/*

#### Crie uma pasta para gravar os dados de saÃ­da no sistema de arquivos local:

mkdir dsa-dados-saida-local

#Entre na pasta:

cd dsa-dados-saida-local

#### Copie os arquivos de saÃ­da do sistema de arquivos distribuÃ­do para o sistema de arquivos local:

hdfs dfs -get /user/hadoop/dsa-dados-saida-dfs/*


---

### Tarefa 5 - Combinar os Arquivos de SaÃ­da e Obter o Resultado do Pipeline no Cluster EMR

#### Acesse a pasta com os dados de saÃ­da no sistema local no Cluster EMR:

cd dsa-dados-saida-local

#Crie o arquivo sh:

vi combina_json.sh

#### Coloque no arquivo o conteÃºdo abaixo para combinar os arquivos JSON e gerar um Ãºnico arquivo de saÃ­da:
```
#!/bin/bash
# Encontra todos os arquivos JSON e os passa para jq para combinaÃ§Ã£o em um Ãºnico array
find . -name "*.json" -print0 | xargs -0 jq -c '.' | jq -s '.' > dsa-resultado.json

# Altere a permissÃ£o do arquivo para tornÃ¡-lo um executÃ¡vel:
chmod +x combina_json.sh

# Execute o script:
./combina_json.sh

# Verifique o resultado:
cat dsa-resultado.json

# Zip do arquivo:
zip dsa-resultado.zip dsa-resultado.json

# Copia o arquivo zip para o S3: EndereÃ§o "aws-logs-890582101704-us-east-2" retirado da pÃ¡gina do Amazon S3 (URI do S3)
aws s3 cp dsa-resultado.zip s3://aws-logs-890582101704-us-east-2/elasticmapreduce/dsa-resultado.zip
```


---
## Contato

Se tiver dÃºvidas ou sugestÃµes sobre o projeto, entre em contato comigo:

- ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/henrique-k-32967a2b5/)
- ğŸ± [GitHub](https://github.com/henriquekurata?tab=overview&from=2024-09-01&to=2024-09-01)
