# üöÄ ***Processamento de dados com Apache Spark utilizando linguagem Python***

## üìñ **Descri√ß√£o do Projeto:**
Este projeto envolve a configura√ß√£o de um cluster Apache Spark usando Docker e a execu√ß√£o de pipelines de Machine Learning em Python. Os pipelines processam diferentes datasets com modelos de regress√£o linear e clustering, explorando a capacidade do Spark de realizar processamento distribu√≠do.




## üõ†Ô∏è Ferramentas Utilizadas:
- Apache Spark
- Python
- Docker

## üìã **Descri√ß√£o do Processo**
O projeto configura um cluster Apache Spark composto por um master, workers e um servidor de hist√≥rico de eventos, utilizando cont√™ineres Docker. Com o cluster em funcionamento, s√£o submetidos tr√™s pipelines de Machine Learning que realizam diferentes tarefas de processamento e an√°lise de dados.

1. Configura√ß√£o de um cluster Spark com Docker.
2. Submiss√£o de pipelines de Machine Learning para o processamento distribu√≠do.
3. Execu√ß√£o de algoritmos de Regress√£o Linear e K-means no cluster.
4. Armazenamento dos resultados em arquivos CSV.


## üíª **Comandos:** 

### Dockerfile (Cria√ß√£o da imagem do Spark)

#Dockerfile (Detalhes do sistema operacional)

```
# Imagem do SO usada como base
FROM python:3.10-bullseye as spark-base

# Atualiza o SO e instala pacotes
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      sudo \
      curl \
      vim \
      nano \
      unzip \
      rsync \
      openjdk-11-jdk \
      build-essential \
      software-properties-common \
      ssh && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Vari√°veis de ambiente
ENV SPARK_HOME=${SPARK_HOME:-"/opt/spark"}
ENV HADOOP_HOME=${HADOOP_HOME:-"/opt/hadoop"}

# Cria as pastas
RUN mkdir -p ${HADOOP_HOME} && mkdir -p ${SPARK_HOME}
WORKDIR ${SPARK_HOME}

# Download do arquivo de bin√°rios do Spark
RUN curl https://dlcdn.apache.org/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz -o spark-3.4.1-bin-hadoop3.tgz \
 && tar xvzf spark-3.4.1-bin-hadoop3.tgz --directory /opt/spark --strip-components 1 \
 && rm -rf spark-3.4.1-bin-hadoop3.tgz

# Prepara o ambiente com PySpark
FROM spark-base as pyspark

# Instala as depend√™ncias Python
COPY requirements/requirements.txt .
RUN pip3 install -r requirements.txt

# Mais vari√°veis de ambiente
ENV PATH="/opt/spark/sbin:/opt/spark/bin:${PATH}"
ENV SPARK_HOME="/opt/spark"
ENV SPARK_MASTER="spark://spark-master:7077"
ENV SPARK_MASTER_HOST spark-master
ENV SPARK_MASTER_PORT 7077
ENV PYSPARK_PYTHON python3

# Copia o arquivo de configura√ß√£o do Spark para a imagem
COPY conf/spark-defaults.conf "$SPARK_HOME/conf"

# Permiss√µes
RUN chmod u+x /opt/spark/sbin/* && \
    chmod u+x /opt/spark/bin/*

# Vari√°vel PYTHONPATH
ENV PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH

# Copia o script de inicializa√ß√£o dos servi√ßos para a imagem
COPY entrypoint.sh .

# Ajusta o privil√©gio
RUN chmod +x entrypoint.sh

# Executa o script quando inicializar um container
ENTRYPOINT ["./entrypoint.sh"]



### docker-compose.yml (Arquivo para cria√ß√£o dos containers docker do Spark)
version: '3.8'

services:
  spark-master:
    container_name: dsa-spark-master
    build: .
    image: dsa-spark-image
    entrypoint: ['./entrypoint.sh', 'master']
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8080" ]
      interval: 5s
      timeout: 3s
      retries: 3
    volumes:
      - ./dados:/opt/spark/data
      - ./jobs:/opt/spark/apps
      - spark-logs:/opt/spark/spark-events
    env_file:
      - .env.spark
    ports:
      - '9090:8080'
      - '7077:7077'


  spark-history-server:
    container_name: dsa-spark-history
    image: dsa-spark-image
    entrypoint: ['./entrypoint.sh', 'history']
    depends_on:
      - spark-master
    env_file:
      - .env.spark
    volumes:
      - spark-logs:/opt/spark/spark-events
    ports:
      - '18080:18080'

  spark-worker:
    image: dsa-spark-image
    entrypoint: ['./entrypoint.sh', 'worker']
    depends_on:
      - spark-master
    env_file:
      - .env.spark
    volumes:
      - ./dados:/opt/spark/data
      - ./jobs:/opt/spark/apps
      - spark-logs:/opt/spark/spark-events

volumes:
  spark-logs:

```

---

### Arquivos de configura√ß√µes do Spark


#### conf > spark-defaults.conf
```
spark.master                           spark://spark-master:7077
spark.eventLog.enabled                 true
spark.eventLog.dir                     /opt/spark/spark-events
spark.history.fs.logDirectory          /opt/spark/spark-events
```

#### requeriments
```
#
# This file is autogenerated by pip-compile-multi
# To update, run:
#
#    pip-compile-multi
#
appnope==0.1.3
    # via ipython
asttokens==2.2.1
    # via stack-data
backcall==0.2.0
    # via ipython
decorator==5.1.1
    # via ipython
executing==1.2.0
    # via stack-data
ipython==8.7.0
    # via -r requirements/requirements.in
jedi==0.18.2
    # via ipython
matplotlib-inline==0.1.6
    # via ipython
numpy==1.24.0
    # via
    #   -r requirements/requirements.in
    #   pandas
    #   pyarrow
pandas==1.5.2
    # via -r requirements/requirements.in
parso==0.8.3
    # via jedi
pexpect==4.8.0
    # via ipython
pickleshare==0.7.5
    # via ipython
prompt-toolkit==3.0.36
    # via ipython
ptyprocess==0.7.0
    # via pexpect
pure-eval==0.2.2
    # via stack-data
py4j==0.10.9.5
    # via pyspark
pyarrow==10.0.1
    # via -r requirements/requirements.in
pygments==2.13.0
    # via ipython
pyspark==3.3.1
    # via -r requirements/requirements.in
python-dateutil==2.8.2
    # via pandas
pytz==2022.7
    # via pandas
six==1.16.0
    # via python-dateutil
stack-data==0.6.2
    # via ipython
traitlets==5.8.0
    # via
    #   ipython
    #   matplotlib-inline
wcwidth==0.2.5
    # via prompt-toolkit
```

#### env.spark

```
SPARK_NO_DAEMONIZE=true
```

#### entrypoint

```
#!/bin/bash

SPARK_WORKLOAD=$1

echo "SPARK_WORKLOAD: $SPARK_WORKLOAD"

if [ "$SPARK_WORKLOAD" == "master" ];
then
  start-master.sh -p 7077
elif [ "$SPARK_WORKLOAD" == "worker" ];
then
  start-worker.sh spark://spark-master:7077
elif [ "$SPARK_WORKLOAD" == "history" ]
then
  start-history-server.sh
fi

```
---

### Criando os containers Spark

Atrav√©s do CMD, navegar at√© a pasta onde est√£o os arquivos de 

configura√ß√µes acima e executar: 

#### Inicializar o cluster
```
docker-compose up -d --scale spark-worker=3
```

#Visualizar os logs
```
docker-compose logs
```
---

### Executando os pipelines (A partir do CMD da m√°quina local):
```
docker exec dsa-spark-master spark-submit --master spark://

spark-master:7077 ./apps/pipeline1.py
```

```
docker exec dsa-spark-master spark-submit --master spark://

spark-master:7077 ./apps/pipeline2.py
```

```
docker exec dsa-spark-master spark-submit --master spark://

spark-master:7077 ./apps/pipeline3.py
```
---

### Derrubando o cluster
```
docker-compose down --volumes --remove-orphans
```
---

### Spark Master

http://localhost:9090

### History Server
http://localhost:18080

---

### Amostra dos dados utilizados

#### dataset1

-9.490009878824548 1:0.4551273600657362 2:0.36644694351969087 
3:-0.38256108933468047

0.2577820163584905 1:0.8386555657374337 2:-0.1270180511534269 3:0.499812362510895

-4.438869807456516 1:0.5025608135349202 2:2.3039160484883194 3:-0.029917319872142175

-19.782762789614537 1:-0.06796133581083447 2:-1.930133447872727 3:-0.4384506889290802

-7.966593841555266 1:0.6452695693733686 2:-1.7119709588890757 3:0.28680207469161273



#### dataset2

atributo1,atributo2,label

cat,10.0,8.0

dog,12.0,7.0

cat,13.0,5.5

dog,11.0,6.0


#### dataset3

idade,renda_anual,pontua√É¬ß√É¬£o_gastos

25,40000,60

35,80000,30

40,60000,50

20,30000,70

30,50000,60

---

### Jobs

#### Executando o Pipeline 1 de Machine Learning no Cluster Spark

```py
# Imports
from pyspark.sql import SparkSession
from pyspark.ml.regression import LinearRegression
from pyspark.sql import Row

# Inicializa o Spark Session
spark = SparkSession.builder.appName("DSAPipeline1").getOrCreate()

# Carrega os dados
training = spark.read.format("libsvm").load("/opt/spark/data/dataset1.txt")

# Inicializa o modelo de Regress√£o Linear
lr = LinearRegression(featuresCol = 'features', labelCol = 'label', maxIter = 10, regParam = 0.3, elasticNetParam = 0.8)

# Treina o modelo
lrModel = lr.fit(training)

# Imprime os coeficientes e intercepta para a regress√£o linear
print("Coeficientes: " + str(lrModel.coefficients))
print("Intercepto: " + str(lrModel.intercept))

# Converte os coeficientes para um DataFrame e salva em um arquivo CSV
coef = [Row(Coeficientes = float(i)) for i in lrModel.coefficients.toArray()]
df = spark.createDataFrame(coef)
df.coalesce(1).write.format('com.databricks.spark.csv').mode("overwrite").option('header', 'true').save('/opt/spark/data/resultadop1')

# Para fechar a Spark Session quando a aplica√ß√£o terminar
spark.stop()

```

#### Executando o Pipeline 2 de Machine Learning no Cluster Spark
```py
#Imports
from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml import Pipeline
from pyspark.sql.functions import col
from pyspark.sql.types import FloatType

# Inicializa o Spark Session
spark = SparkSession.builder.appName("DSAPipeline2").getOrCreate()

# Carrega os dados
df = spark.read.format("csv").option("header", "true").load("/opt/spark/data/dataset2.csv")

# Converte as colunas 'atributo2' e 'label' para FloatType
df = df.withColumn("atributo2", col("atributo2").cast(FloatType()))
df = df.withColumn("label", col("label").cast(FloatType()))

# Indexa a coluna 'atributo1'
indexer = StringIndexer(inputCol = "atributo1", outputCol = "atributo1Index")

# Assembla as features 'atributo1Index' e 'atributo2' em um vetor
assembler = VectorAssembler(inputCols = ["atributo1Index", "atributo2"], outputCol = "features")

# Define o modelo de regress√£o linear
lr = LinearRegression(featuresCol = "features", labelCol = "label", maxIter = 10, regParam = 0.3, elasticNetParam = 0.8)

# Define o pipeline
pipeline = Pipeline(stages = [indexer, assembler, lr])

# Treina o modelo
model = pipeline.fit(df)

# Faz as previs√µes
predictions = model.transform(df)

# Salva as previs√µes em um arquivo CSV
predictions.select("prediction").coalesce(1).write.format('com.databricks.spark.csv').mode("overwrite").option('header', 'true').save('/opt/spark/data/resultadop2')

# Para fechar a Spark Session quando a aplica√ß√£o terminar
spark.stop()
```



#### Executando o Pipeline 3 de Machine Learning no Cluster Spark
```py
#Imports
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.clustering import KMeans
from pyspark.ml import Pipeline
from pyspark.sql.functions import col
from pyspark.sql.types import FloatType

# Inicializa o Spark Session
spark = SparkSession.builder.appName("DSAPipeline3").getOrCreate()

# Carrega os dados
df = spark.read.format("csv").option("header", "true").load("/opt/spark/data/dataset3.csv")

# Converte as colunas 'idade', 'renda_anual' e 'pontua√ß√£o_gastos' para FloatType
df = df.withColumn("idade", col("idade").cast(FloatType()))
df = df.withColumn("renda_anual", col("renda_anual").cast(FloatType()))
df = df.withColumn("pontua√ß√£o_gastos", col("pontua√ß√£o_gastos").cast(FloatType()))

# Assembla as features 'idade', 'renda_anual' e 'pontua√ß√£o_gastos' em um vetor
assembler = VectorAssembler(inputCols = ["idade", "renda_anual", "pontua√ß√£o_gastos"], outputCol = "features")

# Define o modelo K-means
kmeans = KMeans(featuresCol = "features", k = 3)

# Define o pipeline
pipeline = Pipeline(stages = [assembler, kmeans])

# Treina o modelo
model = pipeline.fit(df)

# Faz as previs√µes (atribui os dados a clusters)
predictions = model.transform(df)

# Salva as previs√µes em um arquivo CSV
predictions.select("prediction").coalesce(1).write.format('com.databricks.spark.csv').mode("overwrite").option('header', 'true').save('/opt/spark/data/resultadop3')

# Para fechar a Spark Session quando a aplica√ß√£o terminar
spark.stop()
```

---

### Otimizando o cluster Spark

1. Otimizar a mem√≥ria 

2. Utilizar armazenamento distribu√≠do com HDFS

3. Ativar o modo Yarn (No item anterior foi realizado com o modo standalone)

OBS: Para o armazenamento distribu√≠do precisamos aplicar Hadoop ao Cluster:

---

### Dockerfile

#Cria Imagem 
```py
# Imagem do SO usada como base
FROM python:3.10-bullseye as spark-base

# Atualiza o SO e instala pacotes
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      sudo \
      curl \
      vim \
      nano \
      unzip \
      rsync \
      openjdk-11-jdk \
      build-essential \
      software-properties-common \
      ssh && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

# Vari√°veis de ambiente
ENV SPARK_HOME=${SPARK_HOME:-"/opt/spark"}
ENV HADOOP_HOME=${HADOOP_HOME:-"/opt/hadoop"}

# Cria as pastas
RUN mkdir -p ${HADOOP_HOME} && mkdir -p ${SPARK_HOME}
WORKDIR ${SPARK_HOME}

# Download do arquivo de bin√°rios do Spark
 RUN curl https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz -o spark-3.4.1-bin-hadoop3.tgz \
 && tar xvzf spark-3.4.1-bin-hadoop3.tgz --directory /opt/spark --strip-components 1 \
 && rm -rf spark-3.4.1-bin-hadoop3.tgz


# Download do arquivo de bin√°rios do Hadoop
RUN curl https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz -o hadoop-3.3.6.tar.gz \
 && tar xfz hadoop-3.3.6.tar.gz --directory /opt/hadoop --strip-components 1 \
 && rm -rf hadoop-3.3.6.tar.gz

# Prepara o ambiente com PySpark
FROM spark-base as pyspark

# Instala as depend√™ncias Python
RUN pip3 install --upgrade pip
COPY requirements/requirements.txt .
RUN pip3 install -r requirements.txt

# Vari√°vel de ambiente do JAVA_HOME
ENV JAVA_HOME="/usr/lib/jvm/java-11-openjdk-amd64"

# Adiciona os bin√°rios do Spark, Hadoop e Java no PATH
ENV PATH="$SPARK_HOME/sbin:/opt/spark/bin:${PATH}"
ENV PATH="$HADOOP_HOME/bin:$HADOOP_HOME/sbin:${PATH}"
ENV PATH="${PATH}:${JAVA_HOME}/bin"

# Vari√°veis de ambiente
ENV SPARK_MASTER="spark://spark-master:7077"
ENV SPARK_MASTER_HOST spark-master
ENV SPARK_MASTER_PORT 7077
ENV PYSPARK_PYTHON python3
ENV HADOOP_CONF_DIR="$HADOOP_HOME/etc/hadoop"

# Hadoop native library 
ENV LD_LIBRARY_PATH="$HADOOP_HOME/lib/native:${LD_LIBRARY_PATH}"

# Usu√°rio do HDFS e Yarn
ENV HDFS_NAMENODE_USER="root"
ENV HDFS_DATANODE_USER="root"
ENV HDFS_SECONDARYNAMENODE_USER="root"
ENV YARN_RESOURCEMANAGER_USER="root"
ENV YARN_NODEMANAGER_USER="root"

# Adiciona JAVA_HOME en haddop-env.sh
RUN echo "export JAVA_HOME=${JAVA_HOME}" >> "$HADOOP_HOME/etc/hadoop/hadoop-env.sh"

# Copia os arquivos de configura√ß√£o
COPY yarn/spark-defaults.conf "$SPARK_HOME/conf/"
COPY yarn/*.xml "$HADOOP_HOME/etc/hadoop/"

# Faz com que os bin√°rios sejam execut√°veis
RUN chmod u+x /opt/spark/sbin/* && \
    chmod u+x /opt/spark/bin/*

# Python PATH
ENV PYTHONPATH=$SPARK_HOME/python/:$PYTHONPATH

# SSH para autentica√ß√£o sem senha no Hadoop
RUN ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa && \
  cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys && \
  chmod 600 ~/.ssh/authorized_keys

# Copia o arquivo de configura√ß√£o do SSH
COPY ssh/ssh_config ~/.ssh/config

# Entrypoint script
COPY entrypoint.sh .

# Ajusta o privil√©gio
RUN chmod +x entrypoint.sh

EXPOSE 22

# Executa o script quando inicializar um container
ENTRYPOINT ["./entrypoint.sh"]
```

#### ssh
```
Host *
  UserKnownHostsFile /dev/null
  StrictHostKeyChecking no
```


#### capacity-scheduler
```
<configuration>
  <property>
    <name>yarn.scheduler.capacity.root.queues</name>
    <value>default</value>
  </property>
  <property>
    <name>yarn.scheduler.capacity.root.default.capacity</name>
    <value>100</value>
  </property>
  <property>
    <name>yarn.scheduler.capacity.resource-calculator</name>
    <value>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</value>
  </property>
</configuration>
```




#### core-site
```
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://spark-master:8080</value>
    </property>
</configuration>
```




#### hdfs-site
```
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/opt/hadoop/data/nameNode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/opt/hadoop/data/dataNode</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>
</configuration>
```




#### mapred-site
```
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
        <name>mapreduce.reduce.env</name>
        <value>HADOOP_MAPRED_HOME=$HADOOP_HOME</value>
    </property>
    <property>
        <name>yarn.app.mapreduce.am.resource.mb</name>
        <value>1024</value>
    </property>
    <property>
        <name>mapreduce.map.memory.mb</name>
        <value>512</value>
    </property>
    <property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>512</value>
    </property>
</configuration>
```





#### spark-defaults.conf
```
spark.master                     yarn
spark.submit.deployMode          client
spark.driver.memory              512m
spark.executor.memory            512m
spark.yarn.am.memory             1G
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://spark-master:8080/spark-logs
spark.history.provider           org.apache.spark.deploy.history.FsHistoryProvider
spark.history.fs.logDirectory    hdfs://spark-master:8080/spark-logs
spark.yarn.historyServer.address localhost:18080
spark.history.fs.update.interval 10s
spark.history.ui.port            18080
```





#### yarn-site
```
<?xml version="1.0"?>
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>spark-master</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>2</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>2048</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>2048</value>
    </property>
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>512</value>
    </property>

    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
</configuration>

```



#### ENTRYPOINT
```
#!/bin/bash

SPARK_WORKLOAD=$1

echo "SPARK_WORKLOAD: $SPARK_WORKLOAD"

/etc/init.d/ssh start

if [ "$SPARK_WORKLOAD" == "master" ];
then
  hdfs namenode -format

  # Inicializa os processos no master
  hdfs --daemon start namenode
  hdfs --daemon start secondarynamenode
  yarn --daemon start resourcemanager

  # Cria as pastas necess√°rias
  while ! hdfs dfs -mkdir -p /spark-logs;
  do
    echo "Falha ao criar a pasta /spark-logs no hdfs"
  done
  echo "Criada a pasta /spark-logs no hdfs"
  hdfs dfs -mkdir -p /opt/spark/data
  echo "Criada a pasta /opt/spark/data no hdfs"


  # Copia os dados para o HDFS
  hdfs dfs -copyFromLocal /opt/spark/data/* /opt/spark/data
  hdfs dfs -ls /opt/spark/data

elif [ "$SPARK_WORKLOAD" == "worker" ];
then

  # Inicializa processos no worker
  hdfs --daemon start datanode
  yarn --daemon start nodemanager
elif [ "$SPARK_WORKLOAD" == "history" ];
then

  while ! hdfs dfs -test -d /spark-logs;
  do
    echo "spark-logs n√£o existe ainda...criando"
    sleep 1;
  done
  echo "Exit loop"

  # Inicializa o history server
  start-history-server.sh
fi

tail -f /dev/null
```

#Requirements
```
#
# This file is autogenerated by pip-compile-multi
# To update, run:
#
#    pip-compile-multi
#
numpy==1.24.0
pandas==1.5.2
pyspark==3.4.1
```



#### .env.spark
```
SPARK_NO_DAEMONIZE=true
```



#docker-compose.yml
```
version: '3.8'

services:
  spark-master:
    container_name: dsa-spark-master
    build:
      dockerfile: Dockerfile
      context: .
    image: dsa-spark-image
    entrypoint: ['./entrypoint.sh', 'master']
    volumes:
      - ./dados:/opt/spark/data
      - ./jobs:/opt/spark/apps
    env_file:
      - .env.spark
    ports:
      - '9090:8080'
      - '9870:9870'
      - '7077:7077'
      - '8088:8088'


  spark-worker:
    image: dsa-spark-image
    entrypoint: ['./entrypoint.sh', 'worker']
    depends_on:
      - spark-master
    env_file:
      - .env.spark
    volumes:
      - ./dados:/opt/spark/data
      - ./jobs:/opt/spark/apps

  history-server:
    container_name: dsa-spark-history
    image: dsa-spark-image
    entrypoint: ['./entrypoint.sh', 'history']
    depends_on:
      - spark-master
    env_file:
      - .env.spark
    ports:
      - '18080:18080'

```


---


### Executando o Pipeline de Machine Learning no Cluster Spark

```py
# Imports
from pyspark.sql import SparkSession
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml.regression import LinearRegression
from pyspark.ml import Pipeline
from pyspark.sql.functions import col
from pyspark.sql.types import FloatType

# Inicializa o Spark Session
spark = SparkSession.builder.appName("DSAPipeline").getOrCreate()

# Carrega os dados
df = spark.read.format("csv").option("header", "true").load("hdfs://spark-master:8080/opt/spark/data/dataset1.csv")

# Converte as colunas 'atributo2' e 'label' para FloatType
df = df.withColumn("atributo2", col("atributo2").cast(FloatType()))
df = df.withColumn("label", col("label").cast(FloatType()))

# Indexa a coluna 'atributo1'
indexer = StringIndexer(inputCol = "atributo1", outputCol = "atributo1Index")

# Assembla as features 'atributo1Index' e 'atributo2' em um vetor
assembler = VectorAssembler(inputCols = ["atributo1Index", "atributo2"], outputCol = "features")

# Define o modelo de regress√£o linear
lr = LinearRegression(featuresCol = "features", labelCol = "label", maxIter = 10, regParam = 0.3, elasticNetParam = 0.8)

# Define o pipeline
pipeline = Pipeline(stages = [indexer, assembler, lr])

# Treina o modelo
model = pipeline.fit(df)

# Faz as previs√µes
predictions = model.transform(df)

# Salva as previs√µes em um arquivo CSV
predictions.select("prediction").coalesce(1).write.format('com.databricks.spark.csv').mode("overwrite").option('header', 'true').save('/opt/spark/data/resultado')


# Para fechar a Spark Session quando a aplica√ß√£o terminar
spark.stop()

```

---

### Inicializar o cluster
```
docker-compose up -d --scale spark-worker=3
```

---


### Visualizar os logs
```
docker-compose logs
```
---

### Executar os pipelines:
```
docker exec dsa-spark-master spark-submit --master yarn --deploy-mode cluster ./examples/src/main/python/pi.py

docker exec dsa-spark-master spark-submit --master yarn --deploy-mode cluster ./apps/pipeline1.py
```

---

### Derrubar o cluster
```
docker-compose down --volumes --remove-orphans
```
---

### Acesse

http://localhost:9870/ (para informa√ß√µes do cluster de armazenamento HDFS)

http://localhost:8088/ (para informa√ß√µes do gerenciador de recursos Yarn)

---

### Conferindo e visualizando o resultado no cluster HDFS

root@0cb6aed18884:/opt/spark/data# hdfs dfs -ls /opt/spark/data/resultado

root@0cb6aed18884:/opt/spark/data# hdfs dfs -get /opt/spark/data/resultado/part-00000-94e82e67-0619-4517-9ab1-b55e82e29034-c000.csv

root@0cb6aed18884:/opt/spark# cat part-00000-94e82e67-0619-4517-9ab1-b55e82e29034-c000.csv


---
## Contato

Se tiver d√∫vidas ou sugest√µes sobre o projeto, entre em contato comigo:

- üíº [LinkedIn](https://www.linkedin.com/in/henrique-k-32967a2b5/)
- üê± [GitHub](https://github.com/henriquekurata?tab=overview&from=2024-09-01&to=2024-09-01)







